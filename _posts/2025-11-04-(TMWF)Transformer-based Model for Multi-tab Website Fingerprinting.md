---
title: (TMWF)Transformer-based Model for Multi-tab Website Fingerprinting
description: An end-to-end multi-label website fingerprinting attack model, called the Transformer-based Multi-label Website Fingerprinting Attack Model.
author: eaglesfikr
date: 2025-11-04 22:10:00 +0800
categories: [ETC]
tags: [website fingerprinting]
pin: true
math: true
mermaid: true
---

## 摘要：

提出了一种端到端的多标签网站指纹攻击模型，称为基于 Transformer 的多标签网站指纹攻击模型（TMWF）。受计算机视觉中目标检测算法的启发，我们将多标签网站指纹识别视为预测具有最大长度的有序集的问题。通过在检测模型中添加足够数量的单标签查询，并让每个查询从多标签跟踪的不同位置提取 WF 特征，我们的模型的Transformer 架构能够更充分地利用跟踪特征.结合我们新提出的模型训练方法，我们实现了对具有不同数量标签的多标签跟踪的自适应识别。消除了假设———样本中包含的标签数量属于攻击者的先验知识。

## 创新点: 

1.引入了一种新的多标签 WF 攻击训练方法，旨在打破模型对网页数量的先验知识依赖。2.深度学习模型 TMWF 是端到端的，利用了 Transformer 和可学习的位置编码，能够自动从混合多标签工作流中识别出单标签工作流，有效利用了多标签工作流中的重叠部分。3.提出了一套新的评估指标，专门针对我们所提出的WF 攻击模型。

> 在本文中，我们提出了一种基于 Transformer [3] 架构的深度学习检测模型 TMWF，用于多标签浏览行为。TMWF 由两部分组成：DFNet [14] 和 Transformer。我们选择 DFNet 作为特征提取器，因为它在单标签网站识别任务中表现良好。作为典型的 CNN 结构，DFNet 通过局部建模有效地从原始跟踪序列中提取空间特征。Transformer 是一种通用的序列预测架构，其自注意力机制能够有效地对序列中任意元素之间的全局交互进行建模。TMWF 使用 DFNet 获得原始跟踪序列的特征图，并将其作为序列输入到 Transformer 模块中。该模块并行提取每个标签的嵌入，并保持这些嵌入相对于原始多标签跟踪中特定标签的位置。受计算机视觉中的目标检测算法（如 DETR [12]）的启发，我们使用 Nsingle-tab 查询作为 Transformer 解码器的输入，从原始跟踪的全局特征中提取单标签特征。此操作使模型能够从任何输入序列中识别出 N 个子段对应的类别。作为端到端的多标签识别模型，TMWF 不依赖于人工设计的特征，仅需使用原始的多标签跟踪数据作为输入，即可从混合的多标签跟踪数据中识别出每个单独的网页跟踪数据。具体而言，该模型始终输出 N 个预测结果，其中我们将所有未受监控网站类别的预测结果和冗余预测结果归类为“无标签”，仅保留受监控网站类别的预测结果。理想情况下，模型生成的 N个正确的网站预测结果仅包含受监控类别和冗余的无标签类别。值得注意的是,上述输出模式的改变得益于我们新的模型训练方法。Transformer 架构的变革性贡献在于通过我们提出的训练方法**提升了现有多标签 WF 攻击**（如 **BAPM** [42]）的性能。
> 

## 方法

> ？目标检测为什么可以启发WF多标签视为**预测具有最大长度的有序集的问题：**
>
> {: .prompt-tip }

- 作者借鉴物体检测（object detection）在计算机视觉中的思路：将“每个标签页访问行为”视为一个“查询（query）”或“一个目标对象”，模型预测若干个 query，从流量中识别多个访问。 [github.com+1](https://github.com/jzx-bupt/TMWF?utm_source=chatgpt.com)
- 每个 query 对应一个子任务：预测一个访问页面类别 + 其“位置”或“起止”在混合流量中的大致位置（类似检测物体的 bounding box 起止）。

| 目标检测                      | 多标签 WF                                   |
| ----------------------------- | ------------------------------------------- |
| 输入一张图片（包含多个物体）  | 输入一个混合流量序列（包含多个网页）        |
| 输出每个物体的类别 + 位置信息 | 输出每个网页的类别 + 在流量中的位置（片段） |
| 物体数量不固定                | 标签页数量不固定                            |
| 物体彼此可能重叠              | 不同网页流量可能交叉重叠                    |

即使一张图片里实际只有 3 个目标，模型也总是输出固定长度（比如 100 个）预测向量，每个预测代表一个“可能的目标”，再通过 **匹配（matching）** 去掉空的预测。同理，TMWF 把混合流量看成包含若干“网页访问目标”的序列：

模型输出 Q 个“query”向量（比如最多 5~10 个），每个 query 尝试检测一个网站访问。

形式上：

$$
\hat{Y} = \{(\hat{y}_1, \hat{s}_1), (\hat{y}_2, \hat{s}_2), ..., (\hat{y}_Q, \hat{s}_Q)\}
$$

其中：

- $\hat{y}_i$：第 i 个预测的网页类别
- $\hat{s}_i$：该网页在混合流量中的大致起止位置（类似检测框的坐标）
- Q：最大预测数（即“最大长度”）

在训练时，通过类似于 **Hungarian matching**（匈牙利匹配）算法来把预测结果与真实的标签页访问一一对齐，并对未匹配到的 query 视为“无目标 (no object)”——和目标检测一模一样。

当然，目标检测通常输出的是一个“无序集合（unordered set）”，因为物体之间没有顺序；但在 WF 的多标签流量中，各个网页访问是**按时间顺序混合的**，即：第一个网站先产生流量片段 A，第二个网站稍后产生片段 B……因此，TMWF 在建模时保留了序列的时间顺序信息——每个预测 query 在 Transformer 输出中的位置隐含了它对应的时间片段顺序。这就使得输出集合是“有序的（ordered）”。即：

$$
(\hat{y}_1, \hat{s}_1) < (\hat{y}_2, \hat{s}_2) < ...
$$

这种设计让模型更好地对齐混合流量的时间结构。

DETR 的训练方法对于 WF 模型所使用的输入序列来说具有挑战性。在此背景下，BAPM 的设计理念提供了灵感：通过为多头自注意力结构中的每个注意力头分配固定的标签标识符，该模型仅需页面访问序列作为辅助监督，就能自主区分不同页面的特征。

---

其流程图为：

![](https://imgbed.7ingwe1.top/file/1762229198453_2025-11-04_120606.jpg)

### 骨干网络（特征提取）：

> 卷积神经网络（CNN）仍然是构建模型的主要骨干。这是因为 CNN 在捕捉相邻数据包之间的特征方面表现出色，从而能够有效地对局部特征进行建模。然而，由于 CNN 捕捉长距离依赖关系和全局特征的能力有限，这使得在更具挑战性的多标签网站识别任务中难以区分不同页面的特征。{prompt-tip}
> 

在 TMWF 中，我们利用 DFNet 对输入的多标签轨迹进行局部建模。DFNet 将输入的多标签轨迹 wf 转换为WF 特征序列 F。我们把 DF [14] 的主要网络结构（不包括分类头）称为 DFNet。这里，L 和 l 分别表示经过 DFNet 处理前后的序列长度。

### Tranformer编码器（模型训练）

们使用的是标准的 Transformer 编码器。为了对归一化特征序列 F 进行建模以提取 WF 上下文特征序列 Z。尽管骨干网络输出的特征 F 包含位置关系，但 Transformer 在设计上缺乏对位置信息的归纳偏差，无法从输入特征中学习位置信息。为了弥补这一缺陷，我们在骨干网络输出的特
征序列中添加可学习的位置编码 P，并将其和作为 Transformer编码器的输入。

$$
F'=layerNorm(FW+b)+P,P \in b\\
O_i =TransformerEncoder(O_{i-1}),O_0 =F',Z =O_{NE},i=1,2,...NE
$$

### Tranformer解码器（网站识别）

结合选项卡标签查询。尽管 Transformer 编码器在全局建模方面效果显著，但其输出的上下文特征序列无法区分多选项卡跟踪中不同页面的特征。为此，我们引入 Ntab 查询（类似于 DETR中的对象查询）作为 Transformer 解码器的参考，以查询不同页面的指纹特征。我们用  $T_Q∈R^{N× D}$ 表示选项卡标签查询。

同样使用普通的 Transformer 解码器从编码器生成的上下文特征序列 Zoutput 中提取 Npages 的
WF 特征。解码器能够找到标签查询与上下文特征序列之间合理的对齐方式，从而使每个标签查询都能找到其所属页面的指纹特征。

$$
O_i=TransformerDecoder(O_{i-1},T_Q),O_0=z,E_{WF}=O_{NE},i=1,2...NE
$$

分类。我们使用一个分类头来对 Transformer 解码器提取的 N个pages 工作流嵌入进行分类。此分类将每个页面嵌入转换为网站的概率分布，概率最高的网站被视为 TMWF 对该页面
的预测结果。假设总共有 C个网站，则：

$$
Pr=softmax(E_{WF}W_5+b_5)
$$

矩阵 PR 的第 I 行表示第 I 页的网站概率分布。虽然TMWF 会产生 N 个网站预测结果，但我们预计模型实际预测结果中的页面数量通常不会超过 N 个。对于未受监控的网站和冗余的预测结果，我们统一标注为“no-tab”。

将 WF 攻击中的受监控网站与目标检测中的可识别对象进行类比。对于包含不同页面数量的多标签跟踪样本，我们为每个样本分配 N 个标签。当原始标签数量少于 N 时，我们用“无标签”类标签来扩充样本的标签数量。上方的架构图是用 N = 4 的设置时，对于包含两个受监控网站跟踪记录的多标签跟踪样本，理想情况下，模型生成的四个预测中的第一个和第三个将捕获样本中属于受监控网站的片段。对于测试集样本，其包含的目标数量对模型是透明的，模型始终输出 N= 4 个预测。模型所有者只需选择预测类别不是“无标签”的预测作为最终结果。因此，这种方法实现了对目标数量范围（从 0 到 4）的自适应识别。本质上，去除输入样本中关于实际目标数量的先验知识，其代价是生成超出实际标签数量的冗余结果。

## 总结

实际上就是一个DFNet提取，然后使用tranformer的长序列抓取，并行计算，利用Transformer 架构的变革性来提升BAPM。但是也启发我们去使用目标检测的算法，图像切割的内容来得到多标签混合流量轨迹里各个网站的起止点。

### 局限：

- 模型仍然依赖于知道最大标签页数量。在攻击者设置的最大标签页数量 Nlower 超过用户实际访问的标签页数量的情况下，模型可能无法识别完整跟踪中的所有网站。
- 对于完全重叠的跟踪段的效果下降。我们的方法仍然主要依赖于纯段。当用户以并行方式同时访问多个网页时，多个页面的数据包序列将完全混合在一起。与当前 WF 攻击中使用的串行访问假设相比，完全混合的流量跟踪将放大其特征的混淆效果，从而影响模型性能。
- 未评估 TMWF 在使用 WF 防御技术的流量跟踪上的表现，但在这种情况下其性能会大打折扣。